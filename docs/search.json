[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Writing",
    "section": "",
    "text": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nKevin Scott and Ethan Davis\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#work",
    "href": "index.html#work",
    "title": "Kevin Scott",
    "section": "Work",
    "text": "Work\nI research methods to reduce the computational complexity of retrieval-augmented generation systems. My approach combines classical numerical methods with modern machine learning architectures.\nCurrent areas of investigation include dimension reduction through random projection, efficient semantic search, and graph-based retrieval architectures.\nView projects →"
  },
  {
    "objectID": "index.html#writing",
    "href": "index.html#writing",
    "title": "Kevin Scott",
    "section": "Writing",
    "text": "Writing\nTechnical notes on machine learning, optimization, and system design.\nRead articles →"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Kevin Scott",
    "section": "Contact",
    "text": "Contact\nEmail · GitHub · LinkedIn"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "",
    "text": "We present a methodological framework for evaluating dimensionality reduction techniques in dense embedding spaces for Retrieval-Augmented Generation (RAG) systems. Specifically, we examine Discrete Cosine Transform (DCT) and Principal Component Analysis (PCA) applied to the column space of embedding matrices, followed by quantization. Our evaluation protocol centers on retrieval consistency - measuring whether reduced representations retrieve the same contextual chunks as full-dimensional embeddings. This work was conducted in collaboration with Ethan Davis at SAS Institute Inc."
  },
  {
    "objectID": "posts/welcome.html#abstract",
    "href": "posts/welcome.html#abstract",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "",
    "text": "We present a methodological framework for evaluating dimensionality reduction techniques in dense embedding spaces for Retrieval-Augmented Generation (RAG) systems. Specifically, we examine Discrete Cosine Transform (DCT) and Principal Component Analysis (PCA) applied to the column space of embedding matrices, followed by quantization. Our evaluation protocol centers on retrieval consistency - measuring whether reduced representations retrieve the same contextual chunks as full-dimensional embeddings. This work was conducted in collaboration with Ethan Davis at SAS Institute Inc."
  },
  {
    "objectID": "posts/welcome.html#introduction",
    "href": "posts/welcome.html#introduction",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Introduction",
    "text": "Introduction\nDense embeddings from transformer-based models typically operate in high-dimensional spaces (d ∈ {384, 768, 1024, 1536}). While these representations capture rich semantic information, their dimensionality poses computational and storage challenges. This work examines column-space reduction techniques that aim to preserve retrieval quality in RAG pipelines."
  },
  {
    "objectID": "posts/welcome.html#theoretical-framework",
    "href": "posts/welcome.html#theoretical-framework",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Theoretical Framework",
    "text": "Theoretical Framework\n\nColumn Space Reduction\nGiven an embedding matrix E ∈ ℝ^(n×d) where n represents the number of documents and d the embedding dimension, we seek a transformation T: ℝ^d → ℝ^k where k &lt;&lt; d.\n\n\nDiscrete Cosine Transform (DCT)\nThe DCT provides an orthogonal transformation that concentrates signal energy in low-frequency components:\nE’ = E · D^T\nwhere D is the DCT matrix. We retain the first k columns of E’, exploiting the energy compaction property commonly observed in natural signals.\n\n\nPrincipal Component Analysis (PCA)\nPCA identifies the principal axes of variance in the embedding space:\n\nCenter the data: Ê = E - μ\nCompute covariance: C = (1/n) Ê^T Ê\nEigendecomposition: C = VΛV^T\nProject: E’ = Ê · V_k\n\nwhere V_k contains the k eigenvectors with largest eigenvalues."
  },
  {
    "objectID": "posts/welcome.html#quantization-strategies",
    "href": "posts/welcome.html#quantization-strategies",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Quantization Strategies",
    "text": "Quantization Strategies\nPost-reduction quantization further compresses representations:\n\nScalar Quantization\n\nMap continuous values to discrete levels\nB-bit quantization: 2^B levels\nConsider Lloyd-Max quantizer for optimal level placement based on distribution\n\n\n\nVector Quantization\n\nk-means clustering in reduced space\nCodebook size determines compression ratio\nProduct quantization for large-scale applications"
  },
  {
    "objectID": "posts/welcome.html#proposed-evaluation-methodology",
    "href": "posts/welcome.html#proposed-evaluation-methodology",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Proposed Evaluation Methodology",
    "text": "Proposed Evaluation Methodology\n\nRetrieval Consistency Metric\nFor query q, let: - R_full(q, k) = top-k retrieved chunks using full embeddings - R_reduced(q, k) = top-k retrieved chunks using reduced embeddings\nWe define retrieval consistency as:\nRC@k = |R_full(q, k) ∩ R_reduced(q, k)| / k\n\n\nExperimental Protocol\n\nCorpus Preparation: Segment documents into semantically coherent chunks\nEmbedding Generation: Apply pre-trained encoder (e.g., BERT, Sentence-BERT, E5)\nDimensionality Reduction:\n\nFor PCA: Compute on training subset, apply to full corpus\nFor DCT: Direct application of transformation matrix\n\nQuantization: Apply scalar or vector quantization to reduced embeddings\nEvaluation:\n\nSample diverse query set\nCompare retrieval sets between full and reduced representations\nMeasure RC@k for k ∈ {1, 5, 10, 20, 50}\nAnalyze distribution of retrieval rank changes\n\n\n\n\nAdditional Metrics\nBeyond retrieval consistency, consider: - Semantic Similarity Preservation: Correlation between cosine similarities in original vs. reduced space - Computational Efficiency: Indexing time, query latency, memory footprint - Reconstruction Error: ||E - E’T^†||_F where T^† is the pseudo-inverse"
  },
  {
    "objectID": "posts/welcome.html#theoretical-considerations",
    "href": "posts/welcome.html#theoretical-considerations",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Theoretical Considerations",
    "text": "Theoretical Considerations\n\nInformation-Theoretic Perspective\nThe fundamental question: what is the intrinsic dimensionality of semantic embeddings? Rate-distortion theory suggests embeddings contain redundancy that can be exploited.\n\n\nDCT vs PCA: Key Differences\nPCA: - Data-dependent transformation - Optimal for Gaussian-distributed data - Requires eigendecomposition of covariance matrix - Captures global variance structure\nDCT: - Data-independent transformation - Assumes local smoothness in embedding space - O(n log n) computation via FFT - Natural frequency interpretation"
  },
  {
    "objectID": "posts/welcome.html#implementation-considerations",
    "href": "posts/welcome.html#implementation-considerations",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\n\nComputational Complexity\n\nPCA: O(nd²) for covariance computation + O(d³) for eigendecomposition\nDCT: O(nd log d) via FFT implementation\nQuantization: O(nd) for scalar, O(ndk) for k-means vector quantization\n\n\n\nNumerical Stability\nWhen implementing PCA on high-dimensional embeddings:\n# Prefer SVD over eigendecomposition for numerical stability\nU, S, Vt = np.linalg.svd(centered_embeddings, full_matrices=False)\nprincipal_components = Vt[:k].T"
  },
  {
    "objectID": "posts/welcome.html#open-research-questions",
    "href": "posts/welcome.html#open-research-questions",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Open Research Questions",
    "text": "Open Research Questions\n\nOptimal Dimensionality Selection: How to determine k without extensive empirical search?\nQuery-Dependent Reduction: Can we adaptively select dimensions based on query characteristics?\nNon-linear Alternatives: Would manifold learning techniques preserve retrieval quality better?\nEnd-to-End Learning: Can we train embeddings aware of downstream compression?"
  },
  {
    "objectID": "posts/welcome.html#conclusion",
    "href": "posts/welcome.html#conclusion",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThis work outlines a rigorous framework for evaluating dimensionality reduction techniques in RAG systems. The trade-off between computational efficiency and retrieval quality remains an active area of research. Future work should focus on establishing theoretical guarantees and developing adaptive compression strategies."
  },
  {
    "objectID": "posts/welcome.html#acknowledgments",
    "href": "posts/welcome.html#acknowledgments",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis research was conducted in collaboration with Ethan Davis at SAS Institute Inc. We thank the SAS Institute for computational resources and support."
  },
  {
    "objectID": "posts/welcome.html#references",
    "href": "posts/welcome.html#references",
    "title": "Dimensionality Reduction in Embedding Spaces: A Comparative Analysis of DCT and PCA for RAG Systems",
    "section": "References",
    "text": "References\n\nJohnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space.\nJegou, H., Douze, M., & Schmid, C. (2011). Product quantization for nearest neighbor search.\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks.\n\n\nThis methodology paper outlines ongoing research. Implementations and empirical results will be shared upon completion of experiments."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Kevin Scott\nklscott.learning@gmail.com\nLinkedIn · GitHub\nFor research collaborations or technical discussions."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Dimension reduction for retrieval systems using sketching algorithms. Reduces embedding dimensions by 90% while maintaining retrieval accuracy.\n\n\n\nIntelligent document segmentation using metadata-aware boundaries. Improves context preservation in retrieval-augmented generation.\n\n\n\nGraph-based retrieval architecture for multi-hop reasoning. Combines knowledge graphs with autonomous agent systems."
  },
  {
    "objectID": "projects.html#research",
    "href": "projects.html#research",
    "title": "Projects",
    "section": "",
    "text": "Dimension reduction for retrieval systems using sketching algorithms. Reduces embedding dimensions by 90% while maintaining retrieval accuracy.\n\n\n\nIntelligent document segmentation using metadata-aware boundaries. Improves context preservation in retrieval-augmented generation.\n\n\n\nGraph-based retrieval architecture for multi-hop reasoning. Combines knowledge graphs with autonomous agent systems."
  },
  {
    "objectID": "projects.html#open-source",
    "href": "projects.html#open-source",
    "title": "Projects",
    "section": "Open Source",
    "text": "Open Source\nContributing to the machine learning community through optimizations and benchmarking tools.\n\nLangChain optimizations for RAG pipeline efficiency\nVector database performance benchmarks\nFine-tuning utilities for memory-efficient training"
  },
  {
    "objectID": "projects.html#collaboration",
    "href": "projects.html#collaboration",
    "title": "Projects",
    "section": "Collaboration",
    "text": "Collaboration\nWorking with Ethan Davis at SAS Institute on embedding compression techniques. Focus on practical applications of dimension reduction in production systems.\n\nContact me for collaboration opportunities."
  }
]